{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet 8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - [I - Présentation du projet](#I---Présentation-du-projet)\n",
    "- [II - Banque d'images](#II---Banque-d'images)\n",
    "- [III - Code de calcul](#III---Code-de-calcul)\n",
    "- [IV - Chargement des librairies](#IV---Chargement-des-librairies)\n",
    "- [V - Chargement des données depuis S3](#V---Chargement-des-données-depuis-S3)\n",
    "- [VI - Extraction des catégories](#VI---Extraction-des-catégories)\n",
    "- [VII - Lecture des images](#VII---Lecture-des-images)\n",
    "- [VIII - Réduction dimmensionnelle par PCA](#VIII---Réduction-dimmensionnelle-par-PCA)\n",
    "- [VIIII - Exécution du programme](#VIIII---Exécution-du-programme)\n",
    "- [X - Enregistrement dans S3](#X---Enregistrement-dans-S3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I - Présentation du projet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce projet s'inscrit dans le cadre du développement d'une application mobile qui permettrait aux utilisateurs de prendre en photo un fruit et d'obtenir des informations sur ce fruit.\n",
    "\n",
    "L'objectif de ce projet est de développer un environnement Big Data qui comprendra le preprocessing et une étape de réduction de dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II - Banque d'images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le jeu de données est un ensemble d'images de fruits et de labels associés :\n",
    "https://www.kaggle.com/moltean/fruits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III - Code de calcul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les 2 fonctionnalités principales utilisées dans ce script sont les RDD et les udf.\n",
    "Le principe des RDD a été décrit ci-dessus. Nous utilisons également les pyspark dataFrame qui utilisent la technicité des RDD.\n",
    "\n",
    "Quant aux udf ils permettent d'ajouter une nouvelle colonne à un dataFrame, comme étant le résultat d'une fonction appliqué à une colonne existante.\n",
    "\n",
    "Le code de calcul est composé de 6 blocs distincts :\n",
    "- Le chargement des librairies\n",
    "- La fonction de chargement des données qui renvoie un DataFrame contenant le chemin d'accès aux données\n",
    "- La fonction d'exctraction des catégories qui s'utilise via une udf\n",
    "- La fonction de lecture des images qui renvoie un nouveau DataFrame avec une colonne supplémentaire correspondant aux données images\n",
    "- La fonction de réduction dimensionelle par PCA qui renvoie un nouveau DataFrame ajouté d'une colonne correspondant aux données réduites\n",
    "- La fonction main qui execute toutes les fonctions listées ci-dessus et qui enregistre au format parquet les résultats\n",
    "    \n",
    "Le script python est à executer en fournissant un arguement True ou False selon qu'il est executé en local ou sur la plateforme AWS. Dans le cas d'une execution sur la plateforme AWS, la connexion se fait via la librairie boto3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV - Chargement des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType, IntegerType, DoubleType, DataType, FloatType\n",
    "from pyspark.ml.image import ImageSchema\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT, DenseVector\n",
    "\n",
    "\n",
    "# Fonction pour ouvrir l'image à partir de son chemin d'accès\n",
    "from PIL import Image\n",
    "\n",
    "# Librairies classiques\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "\n",
    "# Librairie pour se connecter au service S3 d'AWS\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V - Chargement des données depuis S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datas(folder):\n",
    "    \n",
    "    \"\"\"\n",
    "    Retourne un dataFrame pyspark avec comme colonne la liste des chemins d'accès\n",
    "    de toutes les images se trouvant le dossier folder\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialisation du temps de calcul\n",
    "    start_time = time.time()\n",
    "    \n",
    "    lst_path =  []\n",
    "    \n",
    "    # Connexion à l'espace de stockage S3 d'AWS\n",
    "    session = boto3.session.Session(aws_access_key_id=\"XXXXXXXXXXXX\",\n",
    "                                    aws_secret_access_key=\"YYYYYYYYYY+6uiK9AMoBB8\")\n",
    "    s3_client = session.client(service_name='s3', region_name=\"us-east-1\")\n",
    "\n",
    "    prefix = 'data'\n",
    "    sub_folders = s3_client.list_objects_v2(Bucket=\"mel-calculsdistribues\", Prefix=prefix)\n",
    "\n",
    "    if \"Contents\" not in sub_folders:\n",
    "        print(\"Erreur lors du chargement des images\")\n",
    "        print(\"Le dossier source n'a pas été trouvé\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    for key in sub_folders[\"Contents\"]:\n",
    "\n",
    "        file = key[\"Key\"]\n",
    "        file = file.replace(prefix + \"/\", \"\")\n",
    "        lst_path.append(folder + file)\n",
    "\n",
    "    print(\"Nombre d'images chargées :\", len(lst_path))\n",
    "    # Création d'un RDD à partit de la liste des chemins d'accès aux images\n",
    "    rdd = sc.parallelize(lst_path)\n",
    "    row_rdd = rdd.map(lambda x: Row(x))\n",
    "    # Création d'un dataFrame pyspark à partir d'un RDD\n",
    "    df = spark.createDataFrame(row_rdd, [\"path_img\"])\n",
    "\n",
    "    # Affichage du temps de calcul\n",
    "    print(\"Temps d'execution {:.2f} secondes\".format(time.time() - start_time))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VI - Extraction des catégories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_categ(path):\n",
    "    \n",
    "    \"\"\"\n",
    "    Retourne le nom du dossier dans lequel se trouve l'image,\n",
    "    qui correspond au nom du type de fruits.\n",
    "    \"\"\"\n",
    "    \n",
    "    list_file = path.split(\"/\")\n",
    "    categ = list_file[-2]\n",
    "    \n",
    "    return categ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VII - Lecture des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(df, col_path='path_img', new_size=(20, 20)):\n",
    "    \n",
    "    \"\"\"\n",
    "    Cette fonction prend comme en entrée un dataframe pyspark avec les noms des chemins d'accès aux images, les ouvres\n",
    "    et renvoie le dataframe d'entrée avec une colonne supplémentaire qui est l'image sous forme de liste.\n",
    "    \n",
    "    Paramètres\n",
    "    df(pyspark DataFrame): contient une colonne avec le chemin d'accès aux images\n",
    "    col_path(string): nom de la colonne où récupérer le chemin d'accès aux images\n",
    "    new_size(tuple): nouvelle taille d'image\n",
    "    \"\"\"\n",
    "\n",
    "        \n",
    "    # Traitement spécifique pour l'accès à S3 via la librairie boto3\n",
    "    def get_path(img_path):\n",
    "        img_path = img_path.replace(\"s3://mel-calculsdistribues/\", \"\")\n",
    "        s3 = boto3.resource(\"s3\", region_name='us-east-1')\n",
    "        bucket = s3.Bucket(\"mel-calculsdistribues\")\n",
    "        object = bucket.Object(img_path)\n",
    "        response = object.get()\n",
    "        file_stream = response['Body']\n",
    "        return file_stream\n",
    "        \n",
    "    # Ouvre l'image via la librairie pillow et resize l'image pour des raisons de mémoires\n",
    "    def open_img(img_path, size=new_size):\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        image = image.resize((20, 20))\n",
    "\n",
    "        return image\n",
    "\n",
    "    # Initilisation du temps de calcul\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Retourne l'image correspondante sous forme de liste pour chaque chemin d'accès d'image\n",
    "    # flatten() pour unidimensionnaliser le tableau (images couleurs)\n",
    "    # tolist() car pyspark n'accepte pas le format numpy\n",
    "    ud_f = udf(lambda img_path: np.asarray(open_img(get_path(img_path))).flatten().tolist())\n",
    "\n",
    "    df = df.withColumn('image', ud_f(col_path))\n",
    "\n",
    "    # Affiche le temps de calcul\n",
    "    print(\"Temps d'execution {:.2f} secondes\".format(time.time() - start_time))\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VIII - Réduction dimmensionnelle par PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_transformation(df, n_components=50, col_image='image'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Applique un algorithme de PCA sur l'ensemble des images pour réduire la dimension de chaque image \n",
    "    du jeu de données.\n",
    "    \n",
    "    Paramètres:\n",
    "    df(pyspark dataFrame): contient une colonne avec les données images\n",
    "    n_components(int): nombre de dimensions à conserver\n",
    "    col_image(string): nom de la colonne où récupérer les données images\n",
    "    \"\"\"\n",
    "\n",
    "    # Initilisation du temps de calcul\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Les données images sont converties au format vecteur dense\n",
    "    ud_f = udf(lambda r: Vectors.dense(r), VectorUDT())\n",
    "    df = df.withColumn('image', ud_f('image'))\n",
    "    \n",
    "    standardizer = StandardScaler(inputCol=\"image\", outputCol=\"scaledFeatures\",\n",
    "                                  withStd=True, withMean=True)\n",
    "    model_std = standardizer.fit(df)\n",
    "    df = model_std.transform(df)\n",
    "\n",
    "    # Entrainement de l'algorithme\n",
    "    pca = PCA(k=n_components, inputCol='scaledFeatures', outputCol='pcaFeatures')\n",
    "    model_pca = pca.fit(df)\n",
    "\n",
    "    # Transformation des images sur les k premières composantes\n",
    "    df = model_pca.transform(df)\n",
    "\n",
    "    df = df.filter(df.pcaFeatures.isNotNull())\n",
    "    \n",
    "    # Affiche le temps de calcul\n",
    "    print(\"Temps d'execution {:.2f} secondes\".format(time.time() - start_time))\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VIIII - Exécution du programme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://mel-calculsdistribues/data/\n",
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/mohammed/.ivy2/cache\n",
      "The jars for the packages stored in: /home/mohammed/.ivy2/jars\n",
      "com.amazonaws#aws-java-sdk-pom added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6c92e697-0d19-49f2-b77a-f7e68a211138;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.amazonaws#aws-java-sdk-pom;1.10.15 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.0.0 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.199 in central\n",
      ":: resolution report :: resolve 847ms :: artifacts dl 26ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.199 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-pom;1.10.15 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.0.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6c92e697-0d19-49f2-b77a-f7e68a211138\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/14ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Liste des images---\n",
      "Nombre d'images chargées : 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'execution 16.59 secondes\n",
      "+--------------------------------------------------+\n",
      "|path_img                                          |\n",
      "+--------------------------------------------------+\n",
      "|s3://mel-calculsdistribues/data/apple_6/r0_0.jpg  |\n",
      "|s3://mel-calculsdistribues/data/apple_6/r0_10.jpg |\n",
      "|s3://mel-calculsdistribues/data/apple_6/r0_100.jpg|\n",
      "|s3://mel-calculsdistribues/data/apple_6/r0_102.jpg|\n",
      "|s3://mel-calculsdistribues/data/apple_6/r0_104.jpg|\n",
      "+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "----Extraction des catégories images-----\n",
      "---Chargement des images---\n",
      "Temps d'execution 0.02 secondes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+\n",
      "|            path_img|  categ|               image|\n",
      "+--------------------+-------+--------------------+\n",
      "|s3://mel-calculsd...|apple_6|[255, 255, 255, 2...|\n",
      "|s3://mel-calculsd...|apple_6|[255, 255, 255, 2...|\n",
      "|s3://mel-calculsd...|apple_6|[255, 255, 255, 2...|\n",
      "|s3://mel-calculsd...|apple_6|[255, 255, 255, 2...|\n",
      "|s3://mel-calculsd...|apple_6|[255, 255, 255, 2...|\n",
      "+--------------------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "---Réduction dimmensionnelle---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'execution 367.10 secondes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+--------------------+--------------------+\n",
      "|            path_img|  categ|               image|      scaledFeatures|         pcaFeatures|\n",
      "+--------------------+-------+--------------------+--------------------+--------------------+\n",
      "|s3://mel-calculsd...|apple_6|[255.0,255.0,255....|[0.0,0.0,0.0,0.0,...|[32.6642765091259...|\n",
      "|s3://mel-calculsd...|apple_6|[255.0,255.0,255....|[0.0,0.0,0.0,0.0,...|[31.8495768511679...|\n",
      "|s3://mel-calculsd...|apple_6|[255.0,255.0,255....|[0.0,0.0,0.0,0.0,...|[21.4767784999753...|\n",
      "|s3://mel-calculsd...|apple_6|[255.0,255.0,255....|[0.0,0.0,0.0,0.0,...|[21.1599890188432...|\n",
      "|s3://mel-calculsd...|apple_6|[255.0,255.0,255....|[0.0,0.0,0.0,0.0,...|[20.7566495717913...|\n",
      "+--------------------+-------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Définis le chemin d'accès au dossier des images\n",
    "    # Chemins différents suivant si le script est executé en local ou sur AWS\n",
    "    try :\n",
    "        folder = \"s3://mel-calculsdistribues/data/\"\n",
    "    except :\n",
    "        sys.exit(0)\n",
    "    print(folder)\n",
    "\n",
    "    # Démarre la session Spark\n",
    "    try :\n",
    "        #sc = SparkContext.getOrCreate()\n",
    "        \n",
    "        import os\n",
    "        os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk-pom:1.10.15,org.apache.hadoop:hadoop-aws:3.0.0 pyspark-shell'\n",
    "        \n",
    "        \n",
    "        #sc.setLogLevel('WARN')\n",
    "        spark = SparkSession.builder.appName(\"App_Fruits\")\\\n",
    "            .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\\\n",
    "            .config(\"spark.hadoop.fs.s3a.access.key\", \"XXXXXXXXXX\")\\\n",
    "            .config(\"spark.hadoop.fs.s3a.secret.key\", \"YYYYYYYYYYYYYY+6uiK9AMoBB8\")\\\n",
    "            .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\\\n",
    "            .config(\"spark.hadoop.fs.s3a.multiobjectdelete.enable\",\"false\")\\\n",
    "            .config(\"spark.hadoop.fs.s3a.fast.upload\",\"true\")\\\n",
    "            .config(\"spark.sql.parquet.filterPushdown\", \"true\")\\\n",
    "            .config(\"spark.sql.parquet.mergeSchema\", \"false\")\\\n",
    "            .config(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\\\n",
    "            .config(\"spark.speculation\", \"false\")\\\n",
    "            .config(\"spark.sql.parquet.writeLegacyFormat\", 'true')\\\n",
    "            .getOrCreate()\n",
    "        \n",
    "        sc = spark.sparkContext\n",
    "        sc.setSystemProperty('com.amazonaws.services.enableV4', 'true')\n",
    "        sc._jsc.hadoopConfiguration().set('fs.s3a.endpoint','s3.eu-west-1.amazonaws.com')\n",
    "      \n",
    "    except :\n",
    "        print(\"Erreur à la construction du moteur spark\")\n",
    "\n",
    "    print(\"---Liste des images---\")\n",
    "    df = load_datas(folder)\n",
    "    df.show(5, False)\n",
    "\n",
    "    print(\"----Extraction des catégories images-----\")\n",
    "    udf_categ = udf(extract_categ, StringType())\n",
    "    df = df.withColumn(\"categ\", udf_categ('path_img'))\n",
    "\n",
    "    print(\"---Chargement des images---\")\n",
    "    df = read_images(df)\n",
    "    df.show(5)\n",
    "\n",
    "    print(\"---Réduction dimmensionnelle---\")\n",
    "    df = pca_transformation(df)\n",
    "    df.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X - Enregistrement dans S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.mode('overwrite').format('parquet').save(\"s3a://mel-calculsdistribues/results_local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
